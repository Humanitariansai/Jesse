{
  "name": "Jesse — AI Data Quality Assurance Agent",
  "nodes": [
    {
      "parameters": {},
      "id": "Start",
      "name": "Start",
      "type": "n8n-nodes-base.manualTrigger",
      "typeVersion": 1,
      "position": [100, 300]
    },
    {
      "parameters": {
        "values": {
          "string": [
            { "name": "CONFIDENCE_THRESHOLD", "value": "0.70" },
            { "name": "DUP_SIM_THRESHOLD", "value": "0.86" },
            { "name": "MAX_ITEMS", "value": "5" },
            { "name": "DISCORD_CHANNEL_ID", "value": "" },
            { "name": "LANGUAGE", "value": "en" },
            { "name": "REGION", "value": "United States" }
          ]
        }
      },
      "id": "Config",
      "name": "Config",
      "type": "n8n-nodes-base.set",
      "typeVersion": 2,
      "position": [300, 300]
    },
    {
      "parameters": {
        "content": "={\n  \"anomaly_rules\": [\n    \"title/company must be non-empty\", \n    \"posted_date within 3y\", \n    \"skills non-empty if description mentions 'required'\"\n  ],\n  \"dup_policy\": {\n    \"signature_fields\": {\n      \"job\": [\"title\", \"company\", \"location\"],\n      \"syllabus\": [\"course_title\", \"instructor\", \"term\"],\n      \"skill\": [\"skill\", \"definition\"]\n    }, \n    \"normalize\": [\n      \"lowercase\",\n      \"strip_punct\",\n      \"collapse_whitespace\"\n    ], \n    \"sim_threshold\": 0.86\n  },\n  \"uncertainty_policy\": {\n    \"fallback_estimator\": \"coverage-based\", \n    \"weights\": {\n      \"field_coverage\": 0.6, \n      \"model_confidence\": 0.4\n    }\n  },\n  \"temporal_policy\": {\n    \"window_days\": 540, \n    \"drift_terms\": [\"definition\", \"synonyms\", \"related\"], \n    \"flag_if_jaccard_drop_below\": 0.55\n  }\n}",
        "options": {}
      },
      "id": "DemoLLMPlan",
      "name": "Demo LLM Plan",
      "type": "n8n-nodes-base.textTemplate",
      "typeVersion": 1,
      "position": [500, 300]
    },
    {
      "parameters": {
        "functionCode": "// Parse the controller plan\nlet plan;\ntry {\n  const planData = $node['DemoLLMPlan'].json;\n  if (typeof planData === 'string') {\n    plan = JSON.parse(planData);\n  } else {\n    plan = planData;\n  }\n} catch (e) {\n  console.log('Error parsing plan:', e.message);\n  plan = {\n    anomaly_rules: [\"title/company must be non-empty\"],\n    dup_policy: {\n      signature_fields: {\n        job: [\"title\", \"company\"],\n        syllabus: [\"course_title\"],\n        skill: [\"skill\"]\n      }\n    }\n  };\n}\n\nreturn [{ json: plan }];"
      },
      "id": "ParsePlan",
      "name": "Parse Controller Plan",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [700, 300]
    },
    {
      "parameters": {
        "content": "=[\n  // Sample job postings\n  {\n    \"type\": \"job\",\n    \"record\": {\n      \"id\": \"job-123\",\n      \"title\": \"Senior Data Scientist\",\n      \"company\": \"Example Tech\",\n      \"location\": \"San Francisco, CA\",\n      \"posted_date\": \"2025-06-15\",\n      \"employment_type\": \"Full-time\",\n      \"description\": \"We are looking for a senior data scientist with experience in machine learning and data analysis.\",\n      \"requirements\": [\"5+ years experience\", \"PhD or MS in Computer Science or related field\"],\n      \"skills\": [\"Python\", \"TensorFlow\", \"SQL\", \"Data Visualization\"]\n    }\n  },\n  // Sample syllabus\n  {\n    \"type\": \"syllabus\",\n    \"record\": {\n      \"id\": \"syllabus-456\",\n      \"course_title\": \"Introduction to Machine Learning\",\n      \"instructor\": \"Dr. Jane Smith\",\n      \"term\": \"Fall 2025\",\n      \"description\": \"This course covers the fundamentals of machine learning algorithms and applications.\",\n      \"topics\": [\"Supervised Learning\", \"Neural Networks\", \"Model Evaluation\"],\n      \"textbooks\": [\"Machine Learning: A Probabilistic Perspective by Kevin Murphy\"]\n    }\n  },\n  // Sample skill\n  {\n    \"type\": \"skill\",\n    \"record\": {\n      \"id\": \"skill-789\",\n      \"skill\": \"Natural Language Processing\",\n      \"definition\": \"The field of computer science focused on programming computers to process and analyze large amounts of natural language data.\",\n      \"synonyms\": [\"NLP\", \"Computational Linguistics\"],\n      \"related\": [\"Machine Learning\", \"Deep Learning\", \"Text Mining\"]\n    }\n  },\n  // Duplicate job posting (intentional near-duplicate for testing)\n  {\n    \"type\": \"job\",\n    \"record\": {\n      \"id\": \"job-124\",\n      \"title\": \"Senior Data Scientist\",\n      \"company\": \"Example Tech\",\n      \"location\": \"San Francisco, CA (Remote)\",\n      \"posted_date\": \"2025-06-16\",\n      \"employment_type\": \"Full-time\",\n      \"description\": \"We are looking for a senior data scientist with experience in ML and data analysis.\",\n      \"requirements\": [\"5+ years experience\", \"PhD or MS in Computer Science\"],\n      \"skills\": [\"Python\", \"TensorFlow\", \"SQL\"]\n    }\n  },\n  // Sample problematic entry with anomalies\n  {\n    \"type\": \"job\",\n    \"record\": {\n      \"id\": \"job-125\",\n      \"title\": \"\",  // Empty title (anomaly)\n      \"company\": \"Tech Company XYZ\",\n      \"location\": \"Boston, MA\",\n      \"posted_date\": \"2023-01-01\",\n      \"employment_type\": \"Contract\",\n      \"description\": \"We require a skilled professional for our team.\", // Mentions 'require' but no skills (anomaly)\n      \"requirements\": [],\n      \"skills\": [] // Empty skills (anomaly)\n    }\n  }\n]",
        "options": {}
      },
      "id": "DemoItems",
      "name": "Demo Data Items",
      "type": "n8n-nodes-base.textTemplate",
      "typeVersion": 1,
      "position": [700, 200]
    },
    {
      "parameters": {
        "functionCode": "// Create a unified stream of items to process\nlet items;\ntry {\n  const itemsData = $node['DemoItems'].json;\n  if (typeof itemsData === 'string') {\n    items = JSON.parse(itemsData);\n  } else if (Array.isArray(itemsData)) {\n    items = itemsData;\n  } else {\n    items = [itemsData];\n  }\n} catch (e) {\n  console.log('Error processing items:', e.message);\n  items = [];\n}\n\n// Create a demo duplicate index for testing\nconst dupIndex = [\n  {\n    canonical_text: \"senior data scientist | example tech | san francisco ca\",\n    hash: 123456789\n  }\n];\n\n// Store duplicate index in flow variables\n$executionContext.setWorkflowStaticData('node', { dupIndex: JSON.stringify(dupIndex) });\n\nconst max = parseInt($node['Config'].json.MAX_ITEMS || 5, 10);\nreturn items.slice(0, max).map(x => ({ json: x }));"
      },
      "id": "BuildStream",
      "name": "Build Unified Stream",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [900, 300]
    },
    {
      "parameters": {
        "batchSize": 1
      },
      "id": "SplitItems",
      "name": "Split Items",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 1,
      "position": [1100, 300]
    },
    {
      "parameters": {
        "functionCode": "// Compute a signature for duplicate detection\nconst plan = $node['ParsePlan'].json || {};\nconst policy = plan.dup_policy || {};\nconst fields = (policy.signature_fields || {})[$json.type] || [];\n\n// Normalize text function\nfunction norm(s) {\n  if (s == null) return '';\n  s = String(s).toLowerCase();\n  s = s.replace(/[^\\p{L}\\p{N}\\s]/gu, '');\n  s = s.replace(/\\s+/g, ' ').trim();\n  return s;\n}\n\n// Extract fields for signature\nconst sigParts = fields.map(f => norm($json.record[f]));\nconst canonical_text = sigParts.join(' | ');\n\n// Simple 32-bit hash\nlet hash = 0;\nfor (let i = 0; i < canonical_text.length; i++) {\n  hash = ((hash << 5) - hash + canonical_text.charCodeAt(i)) | 0;\n}\n\nreturn [{\n  json: {\n    type: $json.type,\n    record: $json.record,\n    signature: {\n      canonical_text,\n      hash\n    }\n  }\n}];"
      },
      "id": "ComputeSignature",
      "name": "Compute Signature",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1300, 300]
    },
    {
      "parameters": {
        "content": "={\n  \"anomalies\": [\n    {\n      \"field\": \"title\",\n      \"issue\": \"Title field is empty\",\n      \"severity\": \"high\"\n    },\n    {\n      \"field\": \"skills\",\n      \"issue\": \"Skills list is empty but description mentions 'required'\",\n      \"severity\": \"medium\"\n    }\n  ],\n  \"schema_ok\": {{$json.record.title ? true : false}}\n}",
        "options": {}
      },
      "id": "DemoAnomalyDetection",
      "name": "Demo Anomaly Detection",
      "type": "n8n-nodes-base.textTemplate",
      "typeVersion": 1,
      "position": [1500, 200]
    },
    {
      "parameters": {
        "functionCode": "// Get the duplicate index from flow variables\nlet dupIndexStr;\ntry {\n  dupIndexStr = $executionContext.getWorkflowStaticData('node').dupIndex;\n} catch (e) {\n  console.log('Error getting dup index:', e.message);\n  dupIndexStr = '[]';\n}\n\nlet dupIndex = [];\ntry {\n  dupIndex = JSON.parse(dupIndexStr);\n} catch (e) {\n  console.log('Error parsing dup index:', e.message);\n  dupIndex = [];\n}\n\n// Get current signature\nconst current = $node['ComputeSignature'].json.signature || {};\n\n// Function to calculate Jaccard similarity between two strings\nfunction jaccard(a, b) {\n  const A = new Set(a.split(' '));\n  const B = new Set(b.split(' '));\n  const inter = [...A].filter(x => B.has(x)).length;\n  const uni = new Set([...A, ...B]).size;\n  return uni ? inter / uni : 0;\n}\n\n// Check for duplicates\nlet isDup = false;\nlet bestSim = 0;\nlet matchSig = null;\n\nfor (const s of dupIndex) {\n  if (s.hash === current.hash) {\n    isDup = true;\n    bestSim = 1;\n    matchSig = s;\n    break;\n  } else {\n    const sim = jaccard(s.canonical_text || '', current.canonical_text || '');\n    if (sim > bestSim) {\n      bestSim = sim;\n      matchSig = s;\n    }\n  }\n}\n\n// Check against threshold\nconst th = parseFloat($node['Config'].json.DUP_SIM_THRESHOLD || 0.86);\nif (bestSim >= th) isDup = true;\n\n// Add current signature to index for future checks\ndupIndex.push(current);\n$executionContext.setWorkflowStaticData('node', { dupIndex: JSON.stringify(dupIndex) });\n\nreturn [{\n  json: {\n    signature: current,\n    is_duplicate: isDup,\n    best_similarity: Number(bestSim.toFixed(4)),\n    match_signature: matchSig\n  }\n}];"
      },
      "id": "DetectDuplicate",
      "name": "Duplicate Detection",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1500, 300]
    },
    {
      "parameters": {
        "content": "={\n  \"uncertainty\": {{$json.type === 'job' && !$json.record.title ? 0.85 : 0.15}},\n  \"notes\": [\n    {{$json.type === 'job' && !$json.record.title ? \"'High uncertainty due to missing title field'\" : \"'Low uncertainty, most required fields present'\"}}\n  ]\n}",
        "options": {}
      },
      "id": "DemoUncertainty",
      "name": "Demo Uncertainty Estimation",
      "type": "n8n-nodes-base.textTemplate",
      "typeVersion": 1,
      "position": [1500, 400]
    },
    {
      "parameters": {
        "content": "={\n  \"temporal_ok\": true,\n  \"drift_score\": 0.1,\n  \"changed_terms\": [],\n  \"notes\": [\"No significant changes detected compared to historical versions\"]\n}",
        "options": {}
      },
      "id": "DemoTemporal",
      "name": "Demo Temporal Analysis",
      "type": "n8n-nodes-base.textTemplate",
      "typeVersion": 1,
      "position": [1500, 500]
    },
    {
      "parameters": {
        "functionCode": "// Parse the results from previous nodes\nlet anomalies = {};\ntry {\n  const anomalyData = $node['DemoAnomalyDetection'].json;\n  if (typeof anomalyData === 'string') {\n    anomalies = JSON.parse(anomalyData);\n  } else {\n    anomalies = anomalyData;\n  }\n} catch (e) {\n  console.log('Error parsing anomalies:', e.message);\n  anomalies = { anomalies: [], schema_ok: true };\n}\n\nlet uncertain = {};\ntry {\n  const uncertaintyData = $node['DemoUncertainty'].json;\n  if (typeof uncertaintyData === 'string') {\n    uncertain = JSON.parse(uncertaintyData);\n  } else {\n    uncertain = uncertaintyData;\n  }\n} catch (e) {\n  console.log('Error parsing uncertainty:', e.message);\n  uncertain = { uncertainty: 0.5, notes: [] };\n}\n\nlet temporal = {};\ntry {\n  const temporalData = $node['DemoTemporal'].json;\n  if (typeof temporalData === 'string') {\n    temporal = JSON.parse(temporalData);\n  } else {\n    temporal = temporalData;\n  }\n} catch (e) {\n  console.log('Error parsing temporal data:', e.message);\n  temporal = { temporal_ok: true, drift_score: 0, changed_terms: [] };\n}\n\n// Get confidence and duplicate info\nconst confField = $json.record?.soft_confidence;\nconst fallbackUnc = Number(uncertain.uncertainty ?? 0);\nconst soft_conf = typeof confField === 'number' ? confField : Math.max(0, 1 - fallbackUnc);\nconst is_dup = $node['DetectDuplicate'].json.is_duplicate;\nconst best_sim = $node['DetectDuplicate'].json.best_similarity;\n\n// Check against thresholds\nconst confThresh = parseFloat($node['Config'].json.CONFIDENCE_THRESHOLD || 0.70);\nconst schemaOk = !!anomalies.schema_ok;\nconst highSev = (anomalies.anomalies || []).some(a => String(a.severity || '').toLowerCase() === 'high');\n\n// Determine if item needs review\nconst needs_review = is_dup || !schemaOk || highSev || soft_conf < confThresh || (temporal.temporal_ok === false);\n\n// Get item type and ID\nconst itemType = $json.type;\nconst rec = $json.record || {};\n\n// Build the final QA record\nconst record = {\n  item_type: itemType,\n  item_id: rec.id || rec.code || rec.title || rec.skill || null,\n  signature: $json.signature,\n  duplicate: {\n    is_duplicate: is_dup,\n    best_similarity: best_sim,\n    match_signature: $node['DetectDuplicate'].json.match_signature || null\n  },\n  anomalies: anomalies.anomalies || [],\n  schema_ok: schemaOk,\n  uncertainty: fallbackUnc,\n  soft_confidence: Number(soft_conf.toFixed(3)),\n  temporal: {\n    ok: temporal.temporal_ok !== false,\n    drift_score: temporal.drift_score ?? null,\n    changed_terms: temporal.changed_terms || []\n  },\n  thresholds: {\n    confidence: confThresh,\n    dup_similarity: parseFloat($node['Config'].json.DUP_SIM_THRESHOLD || 0.86)\n  },\n  needs_review: needs_review,\n  run_ts: new Date().toISOString(),\n  source_excerpt: rec.description || rec.definition || rec.course_title || null\n};\n\nreturn [{ json: { record } }];"
      },
      "id": "BuildQARecord",
      "name": "Build QA JSON Record",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1700, 300]
    },
    {
      "parameters": {
        "path": "=/jesse/qa/{{$json.record.item_type}}_{{$json.record.item_id}}.json",
        "options": {}
      },
      "id": "WriteFile",
      "name": "Write Result to File",
      "type": "n8n-nodes-base.writeBinaryFile",
      "typeVersion": 1,
      "position": [1900, 200]
    },
    {
      "parameters": {
        "conditions": {
          "string": [
            {
              "value1": "={{$node['Config'].json.DISCORD_CHANNEL_ID}}",
              "operation": "isNotEmpty"
            }
          ]
        }
      },
      "id": "CheckDiscordConfig",
      "name": "Check if Discord is Configured",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [1900, 300]
    },
    {
      "parameters": {
        "resource": "message",
        "operation": "send",
        "channelId": "={{$node['Config'].json.DISCORD_CHANNEL_ID}}",
        "message": "QA {{ $json.record.needs_review ? '⚠️ NEEDS REVIEW' : '✅ OK' }} | Type: {{ $json.record.item_type }} | ID: {{ $json.record.item_id }}\nConf: {{ $json.record.soft_confidence }} | Dup: {{ $json.record.duplicate.is_duplicate ? 'YES' : 'NO' }} ({{ $json.record.duplicate.best_similarity }})\nAnomalies: {{ ($json.record.anomalies || []).slice(0,3).map(a => a.field + ': ' + a.issue).join(' | ') || 'None' }}"
      },
      "id": "DiscordNotify",
      "name": "Discord: Notify",
      "type": "n8n-nodes-base.discord",
      "typeVersion": 1,
      "position": [2100, 250]
    },
    {
      "parameters": {
        "conditions": {
          "number": [
            {
              "value1": "={{$node['SplitItems'].context.currentRunIndex}}",
              "operation": "smaller",
              "value2": "={{parseInt($node['Config'].json.MAX_ITEMS || 5) - 1}}"
            }
          ]
        }
      },
      "id": "CheckMoreItems",
      "name": "Check for More Items",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [1900, 400]
    },
    {
      "parameters": {},
      "id": "NextItem",
      "name": "Next Item",
      "type": "n8n-nodes-base.noOp",
      "typeVersion": 1,
      "position": [2100, 400]
    },
    {
      "parameters": {},
      "id": "WorkflowDone",
      "name": "Workflow Complete",
      "type": "n8n-nodes-base.noOp",
      "typeVersion": 1,
      "position": [2100, 500]
    }
  ],
  "connections": {
    "Start": {
      "main": [
        [
          {
            "node": "Config",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Config": {
      "main": [
        [
          {
            "node": "DemoLLMPlan",
            "type": "main",
            "index": 0
          },
          {
            "node": "DemoItems",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "DemoLLMPlan": {
      "main": [
        [
          {
            "node": "ParsePlan",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "ParsePlan": {
      "main": [
        [
          {
            "node": "BuildStream",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "DemoItems": {
      "main": [
        [
          {
            "node": "BuildStream",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "BuildStream": {
      "main": [
        [
          {
            "node": "SplitItems",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "SplitItems": {
      "main": [
        [
          {
            "node": "ComputeSignature",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "ComputeSignature": {
      "main": [
        [
          {
            "node": "DemoAnomalyDetection",
            "type": "main",
            "index": 0
          },
          {
            "node": "DetectDuplicate",
            "type": "main",
            "index": 0
          },
          {
            "node": "DemoUncertainty",
            "type": "main",
            "index": 0
          },
          {
            "node": "DemoTemporal",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "DemoAnomalyDetection": {
      "main": [
        [
          {
            "node": "BuildQARecord",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "DetectDuplicate": {
      "main": [
        [
          {
            "node": "BuildQARecord",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "DemoUncertainty": {
      "main": [
        [
          {
            "node": "BuildQARecord",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "DemoTemporal": {
      "main": [
        [
          {
            "node": "BuildQARecord",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "BuildQARecord": {
      "main": [
        [
          {
            "node": "WriteFile",
            "type": "main",
            "index": 0
          },
          {
            "node": "CheckDiscordConfig",
            "type": "main",
            "index": 0
          },
          {
            "node": "CheckMoreItems",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "CheckDiscordConfig": {
      "main": [
        [
          {
            "node": "DiscordNotify",
            "type": "main",
            "index": 0
          }
        ],
        []
      ]
    },
    "CheckMoreItems": {
      "main": [
        [
          {
            "node": "NextItem",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "WorkflowDone",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "NextItem": {
      "main": [
        [
          {
            "node": "SplitItems",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "settings": {
    "executionOrder": "v1"
  },
  "staticData": null,
  "pinData": {},
  "versionId": "1",
  "meta": {
    "instanceId": "demo-instance",
    "description": "Simplified data quality assurance agent with static demo data to help students learn the workflow structure."
  }
}